{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4610bcd1-f842-4d2c-8dfb-d420c4769295",
   "metadata": {},
   "source": [
    "                                                     Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7abdc9-ebf5-4150-99d1-b3e29b0c08c7",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728007f-2296-4bdc-b123-c73873d15b86",
   "metadata": {
    "tags": []
   },
   "source": [
    "Simple Linear Regression:\n",
    "Definition: Simple linear regression models the relationship between one dependent variable (Y) \n",
    "            and one independent variable (X).\n",
    "            \n",
    "        Equation: The mathematical representation of simple linear regression is: [ Y = C_0 + C_1X + e ]\n",
    "\n",
    "(Y): Dependent Variable (target variable)\n",
    "(X): Independent Variable (input variable)\n",
    "(C_0): Intercept (value of (Y) when (X=0))\n",
    "(C_1): Coefficient (slope)\n",
    "(e): Error term (residuals not explained by the linear regression)\n",
    "\n",
    "    Use Case: Simple linear regression is suitable when there is one clear predictor influencing the outcome. For instance:\n",
    "    Predicting a student’s final exam score based on the number of hours they studied.\n",
    "\n",
    "Multiple Linear Regression:\n",
    "Definition: Multiple linear regression models the relationship between one dependent variable (Y)\n",
    "            and two or more independent variables (X1, X2, X3, …).\n",
    "Equation: The mathematical representation of multiple linear regression is: [ Y = C_0 + C_1X_1 + C_2X_2 + C_3X_3 + \\ldots + C_nX_n + e ]\n",
    "(X_1, X_2, X_3, \\ldots, X_n): Multiple independent variables\n",
    "(C_0, C_1, C_2, C_3, \\ldots, C_n): Coefficients\n",
    "(e): Error term\n",
    "\n",
    "Complexity: Multiple regression is more complex due to multiple relationships being considered simultaneously.\n",
    "Use Case: Multiple linear regression is suitable when multiple factors affect the outcome. For example:\n",
    "Predicting a house price based on features like square footage, number of bedrooms, and neighborhood.\n",
    "Key Differences:\n",
    "Visualization:\n",
    "Simple linear regression: Typically visualized with a 2D scatter plot and a line of best fit.\n",
    "Multiple linear regression: Requires 3D or multi-dimensional space, often represented using partial regression plots.\n",
    "Risk of Overfitting:\n",
    "Simple linear regression: Lower risk, as it deals with only one predictor.\n",
    "Multiple linear regression: Higher risk, especially if too many predictors are used without adequate data.\n",
    "Multicollinearity Concern:\n",
    "Simple linear regression: Not applicable, as there’s only one predictor.\n",
    "Multiple linear regression: A primary concern; correlated predictors can affect model accuracy and interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455919fc-3830-4903-b998-1d996c6e9f83",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf07d8fb-9d92-4541-881e-4765b2b255ec",
   "metadata": {},
   "source": [
    "Linearity Assumption:\n",
    "Assumption: The relationship between the independent variable(s) and the dependent variable is linear.\n",
    "Check:\n",
    "    \n",
    "      Scatter plots: Plot the dependent variable against each independent variable. Look for a roughly linear pattern.\n",
    "        Residual plots: Examine the residuals (differences between actual and predicted values) against the predicted values. \n",
    "        If the residuals are randomly scattered around zero, linearity is likely met.\n",
    "        \n",
    "Independence Assumption:\n",
    "Assumption: Residuals are independent of each other (no autocorrelation).\n",
    "Check:\n",
    "    \n",
    "     Durbin-Watson test: Measures autocorrelation in residuals. A value close to 2 indicates independence.\n",
    "      Time series data: Ensure observations are not temporally dependent. \n",
    "    \n",
    "Homoscedasticity (Equal Variance) Assumption:\n",
    "Assumption: Residuals have constant variance across all levels of the independent variable(s).\n",
    "Check:\n",
    " Residual plots: Look for a consistent spread of residuals across the predicted values.\n",
    "\n",
    "Breusch-Pagan test: Tests for heteroscedasticity. A significant result indicates unequal variance.\n",
    "Normality Assumption:\n",
    "   Assumption: Residuals follow a normal distribution.\n",
    "   Check:\n",
    "        \n",
    "Histogram or Q-Q plot: Examine the distribution of residuals. Ideally, they should resemble a bell curve.\n",
    "Shapiro-Wilk test: Tests normality. A non-significant result suggests normality.\n",
    "No Perfect Multicollinearity Assumption:\n",
    "    \n",
    "Assumption: Independent variables are not perfectly correlated with each other.\n",
    "Check:\n",
    "Correlation matrix: Calculate correlations between independent variables. High correlations may indicate multicollinearity.\n",
    "No Endogeneity Assumption:\n",
    "    \n",
    "Assumption: The error term is not correlated with the independent variables.\n",
    "Check:\n",
    "Instrumental variables: Use instrumental variables to address endogeneity.\n",
    "Granger causality test: Assess causality between variables.\n",
    "No Outliers Assumption:\n",
    "    \n",
    "Assumption: Observations with extreme values do not disproportionately influence the regression.\n",
    "Check:\n",
    " Residual plots: Identify outliers by examining residuals.\n",
    "  Cook’s distance: Detect influential observations."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f74bda8-221d-4437-b21a-5653f672c3c6",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee8dce-0649-4119-95ad-78a08361fb9a",
   "metadata": {},
   "source": [
    "1.Slope (Coefficient):\n",
    "The slope represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X).\n",
    "It indicates the rate of change in the response variable due to a unit change in the predictor.\n",
    "If the slope is positive, it means that as the independent variable increases, the dependent variable tends to increase.\n",
    "   Conversely, a negative slope implies an inverse relationship.\n",
    "2.Intercept (Constant):\n",
    "The intercept ((C_0)) is the predicted value of the dependent variable when the independent variable is zero.\n",
    "It provides the baseline value of the response variable.\n",
    "In practical terms, it’s the value of (Y) when (X) has no effect (e.g., when time is zero or when the predictor is absent).\n",
    "\n",
    "Real-World Example:\n",
    "Suppose we want to predict a student’s final exam score based on the number of hours they studied.\n",
    "We collect data from several students and perform a simple linear regression. Here’s how we interpret the slope and intercept:\n",
    "\n",
    "1.Slope Interpretation:\n",
    "Let’s say our regression equation is: [ \\text{Exam Score} = 50 + 5 \\times \\text{Hours Studied} + e ]\n",
    "The slope (5) indicates that, on average, for every additional hour a student studies, their exam score increases by 5 points.\n",
    "If a student studies 2 hours more than another student, we expect their exam score to be 10 points higher (2 hours × 5 points/hour).\n",
    "\n",
    "2.Intercept Interpretation:\n",
    "The intercept (50) represents the predicted exam score when a student hasn’t studied at all (i.e., zero hours).\n",
    "In this context, it means that if a student didn’t study at all, we would still expect them to score 50 points on\n",
    "the exam (the baseline score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99046a76-5f37-4613-8379-85dc0b934120",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbffc14-5096-4a4c-812a-622e22cc0a36",
   "metadata": {},
   "source": [
    "Gradient Descent in Machine Learning\n",
    "Gradient descent is a fundamental optimization algorithm used to minimize the cost or loss function during model training. \n",
    "It plays a vital role in adjusting model parameters (coefficients) to find optimal values that lead to better predictions. \n",
    "\n",
    "\n",
    "1.Objective:\n",
    "The primary goal of gradient descent is to find the local minimum (or maximum) of a differentiable function.\n",
    "In machine learning, we use gradient descent to minimize a cost function associated with our model.\n",
    "\n",
    "2.Basic Idea:\n",
    "Imagine standing on a hilly landscape (representing the cost function).\n",
    "Your objective is to reach the lowest point (minimum cost).\n",
    "You take steps in the direction that slopes downward (negative gradient) to descend toward the minimum.\n",
    "\n",
    "3.Algorithm Steps:\n",
    "Initialize model parameters (coefficients) randomly.\n",
    "Compute the gradient (partial derivatives) of the cost function with respect to each parameter.\n",
    "Update the parameters by moving in the opposite direction of the gradient (downhill).\n",
    "Repeat the process iteratively until convergence (when the cost function approaches zero or a small value).\n",
    "\n",
    "4.Learning Rate (Alpha):\n",
    "The learning rate (denoted as (\\alpha)) controls the step size during parameter updates.\n",
    "A small (\\alpha) leads to slow convergence, while a large (\\alpha) may overshoot the minimum.\n",
    "Choosing an appropriate (\\alpha) is crucial for efficient convergence.\n",
    "5.Types of Gradient Descent:\n",
    "    \n",
    "Batch Gradient Descent: Computes gradients using the entire dataset. Slow but accurate.\n",
    "Stochastic Gradient Descent (SGD): Uses one random data point per iteration. Faster but noisy.\n",
    "Mini-Batch Gradient Descent: A compromise between batch and SGD, using a small subset of data.\n",
    "6.Applications in Machine Learning:\n",
    "    \n",
    "Model Training: Gradient descent optimizes model parameters (weights) in linear regression, neural networks, and other models.\n",
    "Deep Learning: Backpropagation (used in neural networks) relies on gradient descent for weight updates.\n",
    "Hyperparameter Tuning: Learning rate and other hyperparameters are tuned using gradient descent.\n",
    "\n",
    "Real-World Example:\n",
    "Suppose we’re training a linear regression model to predict house prices based on features like square footage, number of bedrooms,\n",
    "and location. Gradient descent helps us adjust the model’s coefficients (weights) to minimize the mean squared error (our cost function).\n",
    "By iteratively updating the weights, we converge to the best-fit line that minimizes prediction errors."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb862130-7943-4583-84af-19788bfd168b",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11b27df1-7694-4376-ac5d-db438ca9d0ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "1.Simple Linear Regression:\n",
    "Definition: Simple linear regression models the relationship between one dependent variable (Y) and one independent variable (X).\n",
    "    Equation: The mathematical representation of simple linear regression is: [ Y = C_0 + C_1X + e ]\n",
    "    (Y): Dependent Variable (target variable)\n",
    "    (X): Independent Variable (input variable)\n",
    "    (C_0): Intercept (value of (Y) when (X=0))\n",
    "    (C_1): Coefficient (slope)\n",
    "    (e): Error term (residuals not explained by the linear regression)\n",
    "    Use Case: Simple linear regression is suitable when there is one clear predictor influencing the outcome. For example:\n",
    "    Predicting a student’s final exam score based on the number of hours they studied.\n",
    "\n",
    "2.Linear regression\n",
    "\n",
    "Linear regression\n",
    "Multiple Linear Regression:\n",
    "Definition: Multiple linear regression models the relationship between one dependent variable (Y) and two or more \n",
    "independent variables (X1, X2, X3, …).\n",
    "  Equation: The mathematical representation of multiple linear regression\n",
    "    is: [ Y = C_0 + C_1X_1 + C_2X_2 + C_3X_3 + \\ldots + C_nX_n + e ]\n",
    "(X_1, X_2, X_3, \\ldots, X_n): Multiple independent variables\n",
    "(C_0, C_1, C_2, C_3, \\ldots, C_n): Coefficients\n",
    "(e): Error term`\n",
    "Complexity: Multiple regression is more complex due to multiple relationships being considered simultaneously.\n",
    "Use Case: Multiple linear regression is suitable when multiple factors affect the outcome. For example:\n",
    "Predicting a house price based on features like square footage, number of bedrooms, and neighborhood.\n",
    "3.Key Differences:\n",
    "Number of Variables:\n",
    "Simple linear regression: Involves only one independent variable.\n",
    "Multiple linear regression: Incorporates two or more independent variables.\n",
    "Equation Complexity:\n",
    "Simple linear regression: Has a single coefficient for the independent variable.\n",
    "Multiple linear regression: Each independent variable has its own coefficient to account for its impact.\n",
    "Visualization:\n",
    "Simple linear regression: Typically visualized with a 2D scatter plot and a line of best fit.\n",
    "Multiple linear regression: Requires 3D or multi-dimensional space, often represented using partial regression plots."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d920a830-feec-4e01-9e0e-485b7f76a6f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74b53b62-2403-4c70-a5e2-cc5aeaaf83c3",
   "metadata": {},
   "source": [
    "What is Multicollinearity?\n",
    "Definition: Multicollinearity occurs when predictor variables (independent variables) in a regression model\n",
    "are highly correlated with each other.\n",
    "Challenge: When variables are strongly correlated, it becomes difficult to discern their separate effects on the \n",
    "           target variable (dependent variable).\n",
    "Consequences:\n",
    "      Inaccurate Coefficients: Coefficient estimates for correlated variables may not accurately reflect their true impact.\n",
    "Sensitivity: Coefficients become sensitive to small changes in the model.\n",
    "Statistical Significance: Some coefficients may appear statistically insignificant even if the overall model fits well.\n",
    "Detecting Multicollinearity:\n",
    "\n",
    "1.Variance Inflation Factor (VIF):\n",
    "Purpose: VIF assesses the strength of correlation among predictors.\n",
    "Calculation: For each predictor, calculate its VIF using the formula: [ \\text{VIF} = \\frac{1}{1 - R^2} ]\n",
    "(R^2): Coefficient of determination from a regression of the predictor on all other predictors.\n",
    "Threshold: A VIF above 5 or 10 indicates significant multicollinearity.\n",
    "\n",
    "2.Correlation Matrix:\n",
    "Inspect pairwise correlations between predictors.\n",
    "Heatmaps visually represent correlations.\n",
    "Threshold: Correlations above 0.7 or 0.8 are concerning.\n",
    "\n",
    "3.Condition Number:\n",
    "Purpose: Measures the overall collinearity in the model.\n",
    "Calculation: Compute the square root of the ratio of the largest eigenvalue to the smallest eigenvalue of the correlation matrix.\n",
    "Threshold: A condition number above 30 suggests multicollinearity.\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "1.Remove Collinear Variables:\n",
    "Strategy: Drop one of the highly correlated variables.\n",
    "Guideline: Keep the most relevant variable based on domain knowledge or feature importance.\n",
    "2.Combine Variables:\n",
    "Principal Component Analysis (PCA): Create new uncorrelated variables (principal components) from the original predictors.\n",
    "Factor Analysis: Similar to PCA but used for latent variables.\n",
    "3.Regularization Techniques:\n",
    "Ridge Regression (L2 regularization): Shrinks coefficients, reducing multicollinearity impact.\n",
    "Lasso Regression (L1 regularization): Encourages sparsity by setting some coefficients to zero.\n",
    "4.Collect More Data:\n",
    "Solution: Increasing the sample size can help mitigate multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf627450-856c-4bc4-97be-0f5a3bef8efd",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c97d1ea-4735-4867-90a7-e04cfccaac49",
   "metadata": {},
   "source": [
    "Linear Regression:\n",
    "Linear regression is a fundamental statistical technique used to model the relationship between a continuous \n",
    "dependent variable (also known as the response variable) and one or more independent variables (predictors).\n",
    "In linear regression, we aim to find a straight line (a linear equation) that best fits the data points. \n",
    "This line represents the mean change in the dependent variable for a one-unit change in each independent variable.\n",
    "The most common form of linear regression is ordinary least squares (OLS), where the fitted line minimizes the sum of \n",
    "squared differences between the data points and the line.\n",
    "Linear regression assumes a linear relationship between the variables, but it can also model curvature by \n",
    "including interaction effects or using polynomial terms.\n",
    "Despite the name “linear model,” it can handle some degree of curvature.\n",
    "\n",
    "Polynomial Regression:\n",
    "Polynomial regression extends simple linear regression by allowing for non-linear relationships to be modeled.\n",
    "Instead of fitting a straight line, polynomial regression fits a curve to the data. This curve can take various shapes,\n",
    "such as quadratic, cubic, or higher-order polynomials.\n",
    "The relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial \n",
    "in the independent variable(s).\n",
    "\n",
    "For example, a quadratic polynomial regression might have the form:\n",
    "[ y = a_0 + a_1x + a_2x^2 ]\n",
    "Here:\n",
    "(y) represents the dependent variable.\n",
    "(x) represents the independent variable.\n",
    "(a_0), (a_1), and (a_2) are coefficients.\n",
    "Polynomial regression is particularly useful when the data exhibits curvature or when linear regression fails to\n",
    "capture the underlying relationship.\n",
    "By introducing polynomial terms, we can better fit data that doesn’t follow a straight-line pattern.\n",
    "Key Differences:\n",
    "    \n",
    "Linearity:\n",
    "Linear regression assumes a linear relationship between variables.\n",
    "Polynomial regression allows for non-linear relationships by introducing polynomial terms.\n",
    "Model Complexity:\n",
    "    \n",
    "Linear regression is simpler and more straightforward.\n",
    "Polynomial regression increases model complexity due to additional terms.\n",
    "Use Cases:\n",
    "    \n",
    "Linear regression is suitable for continuous dependent variables.\n",
    "Polynomial regression is used when the relationship between predictors and the response is non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b9524-5f16-42b6-b00a-890233d39f01",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3cbfd-5d3e-437a-a6c6-23a9ac1b06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of Polynomial Regression:\n",
    "1.Flexibility in Modeling Curvature:\n",
    "Polynomial regression can capture non-linear relationships between variables.\n",
    "It allows for more flexible modeling by introducing polynomial terms (quadratic, cubic, etc.).\n",
    "Useful when the underlying data exhibits curvature or complex patterns.\n",
    "2.Better Fit for Non-Linear Data:\n",
    "When the true relationship between predictors and the response is not linear, polynomial regression provides a better fit.\n",
    "Linear regression may fail to capture the nuances of non-linear data.\n",
    "3.Higher Order Approximation:\n",
    "By including higher-order terms (e.g., quadratic or cubic), polynomial regression approximates the data more accurately.\n",
    "Linear regression is limited to straight-line approximations.\n",
    "Disadvantages of Polynomial Regression:\n",
    "1.Overfitting:\n",
    "As the degree of the polynomial increases, the model becomes more complex.\n",
    "High-degree polynomials can lead to overfitting, where the model fits noise in the data rather than the true underlying relationship.\n",
    "Regularization techniques (e.g., Ridge or Lasso) can mitigate overfitting.\n",
    "2.Increased Complexity:\n",
    "Polynomial regression introduces additional coefficients for each polynomial term.\n",
    "Interpretability becomes challenging as the model complexity grows.\n",
    "3.Sensitivity to Outliers:\n",
    "Polynomials can be sensitive to outliers.\n",
    "Extreme data points can disproportionately influence the curve.\n",
    "When to Prefer Polynomial Regression:\n",
    "1.Curved Relationships:\n",
    "Use polynomial regression when you suspect a non-linear relationship between variables.\n",
    "For instance, in physics, the relationship between force and displacement often follows a quadratic curve.\n",
    "2.Domain-Specific Knowledge:\n",
    "If domain knowledge suggests a specific polynomial relationship (e.g., physical laws), polynomial regression is appropriate.\n",
    "3.Exploratory Analysis:\n",
    "Polynomial regression can be useful for exploring data before deciding on the appropriate model.\n",
    "It helps identify whether a linear or non-linear relationship exists.\n",
    "4.Small Sample Size:\n",
    "When the sample size is small, polynomial regression may provide a better fit than linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
